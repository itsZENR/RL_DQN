{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN++7F5+pJscDJkibusN8Op",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/itsZENR/RL_DQN/blob/main/RL_DQN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gymnasium"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e0RcdhVl81uD",
        "outputId": "de410c22-1472-4824-eea8-06d5963f27ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting gymnasium\n",
            "  Downloading gymnasium-0.28.1-py3-none-any.whl (925 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m925.5/925.5 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.22.4)\n",
            "Collecting jax-jumpy>=1.0.0 (from gymnasium)\n",
            "  Downloading jax_jumpy-1.0.0-py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.5.0)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium)\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Installing collected packages: farama-notifications, jax-jumpy, gymnasium\n",
            "Successfully installed farama-notifications-0.0.4 gymnasium-0.28.1 jax-jumpy-1.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ej_ow5E-8GhC"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import random\n",
        "import asyncio\n",
        "\n",
        "from tensorflow import device\n",
        "from tensorflow.python.keras.layers import Dense\n",
        "from tensorflow.python.keras import Sequential\n",
        "from tensorflow.python.keras.losses import MeanSquaredError\n",
        "from tensorflow.python.keras.initializers import RandomUniform\n",
        "from collections import deque"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make(\"Acrobot-v1\") # Загружаем среду\n",
        "state = env.reset() # Получаем текущие состояние\n",
        "\n",
        "REPLAY_MEMORY_SIZE = 50_000 # максимальное количиство данных для обучение ии\n",
        "MIN_REPLAY_MEMORY_SIZE = 1000 # минимальное количиство данных для обучение ии\n",
        "UPDATE_TARGET_EVERY = 5 # Через сколько будут обновлятся веса model_target\n",
        "NUM_EPIZODS = 10 # Всево эпизодоа\n",
        "max_steps = 10_000 # максимальное количиство шагов в эпизоде"
      ],
      "metadata": {
        "id": "l7uTkMBz8JE3"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DQNAgent:\n",
        "\n",
        "    def __init__(self):\n",
        "      self.N_action = 2\n",
        "      self.model = self.create_model()\n",
        "      self.target_model = self.create_model()\n",
        "      self.target_model.set_weights(self.model.get_weights())\n",
        "      self.memory = deque(maxlen=REPLAY_MEMORY_SIZE)\n",
        "      self.epsilon = 1.0\n",
        "      self.epsilon_min = 0.001\n",
        "      self.decay_epsilon = -0.0005\n",
        "      self.gamma = 0.98\n",
        "      self.update_target = 0\n",
        "\n",
        "    def create_model(self):\n",
        "        model = Sequential([\n",
        "            Dense(100, activation='relu', kernel_initializer=RandomUniform(minval=-0.3, maxval=0.3)),\n",
        "            Dense(50, activation='relu', kernel_initializer=RandomUniform(minval=-0.3, maxval=0.3)),\n",
        "            Dense(2, activation='linear')\n",
        "        ])\n",
        "        model.compile(optimizer='adam')\n",
        "        model.predict(state)\n",
        "        return model\n",
        "\n",
        "    def update_replay_memory(self, state, state_next, reward, action, done):\n",
        "      self.memory.append([state, state_next, reward, action, done])\n",
        "\n",
        "    def act(self, state):\n",
        "      if random.random() < self.epsilon:\n",
        "        return random.choice(range(3))\n",
        "      else:\n",
        "        action = self.model.predict(state)\n",
        "        return np.argmax(action)\n",
        "\n",
        "    def epsilon_minval(self):\n",
        "      if self.epsilon > self.epsilon_min:\n",
        "        self.epsilon = np.exp(self.decay_epsilon - self.epsilon)\n",
        "      else:\n",
        "        self.epsilon = self.epsilon_min\n",
        "\n",
        "    async def train(self):\n",
        "        if len(self.memory) < MIN_REPLAY_MEMORY_SIZE:\n",
        "            return\n",
        "\n",
        "        queue = asyncio.Queue()\n",
        "        minibatch = random.sample(self.memory, len(self.memory))\n",
        "        task = []\n",
        "\n",
        "        for state, state_next, reward, action, done in minibatch:\n",
        "          task.append(self.train_precess(state, state_next, reward, action, done))\n",
        "\n",
        "        await queue.join()\n",
        "        await asyncio.gather(*task, return_exceptions=True)\n",
        "\n",
        "        self.update_target += 1\n",
        "\n",
        "    def update_weights_target_model(self):\n",
        "      if self.update_target == UPDATE_TARGET_EVERY:\n",
        "            self.target_model.set_weights(self.model.get_weights())\n",
        "            self.update_target = 0\n",
        "\n",
        "    async def train_precess(self, state, state_next, reward, action, done):\n",
        "      if done:\n",
        "          target = reward\n",
        "      else:\n",
        "          target = reward + self.gamma * np.max(self.model.predict(np.array(state)))\n",
        "          target_q = self.target_model.predict(np.array(state_next))\n",
        "          target_q[action] = target\n",
        "          await self.model.fit(np.array(state), np.array(target_q), batch_size=len(self.memory), verbose=1)"
      ],
      "metadata": {
        "id": "0hZ-jjyi9ryi"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent = DQNAgent()\n",
        "victory = 0\n",
        "\n",
        "with device('/gpu:0'):\n",
        "  for epizod in range(NUM_EPIZODS):\n",
        "      state = env.reset()\n",
        "\n",
        "      for step in range(max_steps):\n",
        "        action = agent.act(state)\n",
        "        if action > 2:action = 2\n",
        "        observation, reward, done, info, _ = env.step(int(action))\n",
        "        # print(f'action = {action}, step = {step}')\n",
        "        agent.update_replay_memory(state, observation, reward, action, done)\n",
        "        agent.train()\n",
        "        agent.update_weights_target_model()\n",
        "        agent.epsilon_minval()\n",
        "\n",
        "        state = observation\n",
        "        # print(f'reward = {reward}')\n",
        "\n",
        "        if done:\n",
        "          victory += 1\n",
        "          print('max_step', step)\n",
        "          print(f'victory = {victory}')\n",
        "          break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y_NDwSSx9s2k",
        "outputId": "769b9db8-e278-47f4-a041-b2e534f27610"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-55-a7d2409e8228>:14: RuntimeWarning: coroutine 'DQNAgent.train' was never awaited\n",
            "  agent.train()\n",
            "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n",
            "WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor, but we receive a <class 'tuple'> input: (<tf.Tensor 'ExpandDims:0' shape=(None, 1) dtype=float32>, {})\n",
            "Consider rewriting this model with the Functional API.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "max_step 1216\n",
            "victory = 1\n",
            "max_step 1198\n",
            "victory = 2\n",
            "max_step 756\n",
            "victory = 3\n",
            "max_step 601\n",
            "victory = 4\n",
            "max_step 1164\n",
            "victory = 5\n",
            "max_step 1303\n",
            "victory = 6\n",
            "max_step 1281\n",
            "victory = 7\n",
            "max_step 1256\n",
            "victory = 8\n",
            "max_step 1183\n",
            "victory = 9\n",
            "max_step 1514\n",
            "victory = 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env.close()"
      ],
      "metadata": {
        "id": "7xj0EtkrNyEr"
      },
      "execution_count": 43,
      "outputs": []
    }
  ]
}